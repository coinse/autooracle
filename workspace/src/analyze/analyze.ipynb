{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "from IPython.display import display\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import os \n",
    "import re \n",
    "import pickle\n",
    "\n",
    "from utils.env import EvoD4jEnv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n",
    "from scipy.stats import pearsonr\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_docker_to_local_path(path):\n",
    "    path  = path.replace(\"/root/workspace\",\"/home/coinse/Workspace/autooracle/workspace\")\n",
    "    return path\n",
    "\n",
    "def convert_to_prompt_path(chat_reply_file_path):\n",
    "    if \"chat_reply_mut\" in chat_reply_file_path:\n",
    "        chat_reply_file_path = chat_reply_file_path.replace(\"chat_reply_mut\", \"prompt\")\n",
    "    else:\n",
    "        chat_reply_file_path = chat_reply_file_path.replace(\"chat_reply\", \"prompt\")\n",
    "    prompt_path = chat_reply_file_path.split('/')\n",
    "    del prompt_path[-2]\n",
    "    if prompt_path[-1].endswith(\"reply.txt\"):\n",
    "        prompt_path[-1] = prompt_path[-1].replace(\"reply.txt\",\"query.txt\")\n",
    "    prompt_path = '/'.join(prompt_path)\n",
    "    return prompt_path\n",
    "\n",
    "def extract_label(path):\n",
    "    with open(path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "    lines = file_content.split('\\n')\n",
    "    for line in reversed(lines):\n",
    "        if 'undecidable' in line.lower():\n",
    "            return 'undecidable'\n",
    "        elif 'incorrect' in line.lower():\n",
    "            return 'incorrect'\n",
    "        elif 'correct' in line.lower():\n",
    "            return 'correct'\n",
    "        else:\n",
    "            pass\n",
    "    # If no label is found, return None\n",
    "    return \"None\"\n",
    "\n",
    "def get_docmumet_length(prompt_path):\n",
    "    with open(prompt_path,'r') as f:\n",
    "        prompt = f.readlines()\n",
    "    document = ''\n",
    "    isdocument=False\n",
    "    for line in prompt:\n",
    "        if line.find(\"signature:\") != -1:\n",
    "            isdocument = True\n",
    "            continue\n",
    "        if isdocument:\n",
    "            if line.find(\"```\") != -1:\n",
    "                isdocument= False\n",
    "                continue\n",
    "            document += line.strip()\n",
    "        if not isdocument and len(document) > 0:\n",
    "            break\n",
    "    return len(document)\n",
    "\n",
    "def process_test_info(file_name, chat_reply_file_path):\n",
    "    test_infos = file_name.split('_')\n",
    "    test_class = test_infos[0]\n",
    "    test_no = test_infos[1]\n",
    "    oracle_type = test_infos[2]\n",
    "    document_length = get_docmumet_length(convert_to_prompt_path(chat_reply_file_path))\n",
    "    return test_class, test_no, oracle_type, document_length\n",
    "\n",
    "def get_idx_chat_reply_result(project, version, idx, ts_id, prompt_no, example_num, is_conv):\n",
    "    env = EvoD4jEnv(project, version, idx, ts_id)\n",
    "    chat_reply_org_root_path = convert_docker_to_local_path(os.path.join(env.evosuite_chat_reply_dir, f'prompt{prompt_no}', f'example{example_num}'))\n",
    "    chat_reply_trs_root_path = convert_docker_to_local_path(os.path.join(env.evosuite_chat_reply_transform_dir, f'prompt{prompt_no}', f'example{example_num}'))\n",
    "\n",
    "    idx_test_info = {} # key: test_no, value : [(result1 info), result2, result3, result4, result5]\n",
    "    idx_test_result = {} # \n",
    "    for root_path in [chat_reply_org_root_path, chat_reply_trs_root_path]:\n",
    "        for try_no in range(1, 6):\n",
    "            chat_reply_path = os.path.join(root_path, f'try{try_no}')\n",
    "            for file_name in os.listdir(chat_reply_path):\n",
    "                if is_conv and file_name.endswith(\"1\"):\n",
    "                    continue\n",
    "                chat_reply_file_path = os.path.join(chat_reply_path, file_name)\n",
    "                label = extract_label(chat_reply_file_path)\n",
    "                test_class, test_no, oracle_type, document_length = process_test_info(file_name, chat_reply_file_path)\n",
    "                idx_test_result.setdefault(test_no, []).append(label)\n",
    "                idx_test_info.setdefault(test_no, [test_class, oracle_type, document_length])\n",
    "\n",
    "    return idx_test_info, idx_test_result\n",
    "\n",
    "def all_chat_reply_result(project, version, ts_id, prompt_no, example_no, is_conv):\n",
    "    with open(f'/home/coinse/Workspace/autooracle/workspace/src/select_dataset/{project}_tests_part3.pkl',\"rb\") as fw:\n",
    "        dataset_info = pickle.load(fw)\n",
    "    all_fail_idx_test_info = {}\n",
    "    all_fail_idx_test_result = {}\n",
    "    all_pass_idx_test_info = {}\n",
    "    all_pass_idx_test_result = {}\n",
    "    for idx, [fail_tests, pass_tests] in dataset_info.items():\n",
    "        idx_test_info, idx_test_result = get_idx_chat_reply_result(project, version, str(idx), ts_id, prompt_no, example_no, is_conv)\n",
    "        \n",
    "        fail_test_info = {test_no: info for test_no, info in idx_test_info.items() if test_no in fail_tests}\n",
    "        fail_test_result = {test_no: label for test_no, label in idx_test_result.items() if test_no in fail_tests}\n",
    "\n",
    "        pass_test_info = {test_no: info for test_no, info in idx_test_info.items() if test_no in pass_tests}\n",
    "        pass_test_result = {test_no: label for test_no, label in idx_test_result.items() if test_no in pass_tests}\n",
    " \n",
    "        all_fail_idx_test_info[idx] = fail_test_info\n",
    "        all_fail_idx_test_result[idx] = fail_test_result\n",
    "        all_pass_idx_test_info[idx] = pass_test_info\n",
    "        all_pass_idx_test_result[idx] = pass_test_result\n",
    "    return all_fail_idx_test_info, all_fail_idx_test_result, all_pass_idx_test_info, all_pass_idx_test_result\n",
    "\n",
    "def process_test_info_df(info_dict, result_dict, sample_num, gt_label):\n",
    "    df_rows = []\n",
    "    for idx, tests_dict in info_dict.items():\n",
    "        for test_no, info_list in tests_dict.items():\n",
    "            df_rows.append({\n",
    "                'project': 'Chart',\n",
    "                'version': '1',\n",
    "                'idx': idx,\n",
    "                'ts_id': 'newTS_300',\n",
    "                'test_no': test_no,\n",
    "                'test_class': info_list[0],\n",
    "                'type': info_list[1],\n",
    "                'document_length': info_list[2],\n",
    "                'ground_truth': gt_label,\n",
    "                'attempt_org': result_dict[idx][test_no][:sample_num],\n",
    "                'attempt_trs': result_dict[idx][test_no][5: 5 + sample_num]\n",
    "            })\n",
    "    return pd.DataFrame(df_rows)\n",
    "\n",
    "def score_org(attempt_list):\n",
    "    score = 0\n",
    "    for reply in attempt_list:\n",
    "        if reply == \"correct\":\n",
    "            score += 1\n",
    "        elif reply == \"incorrect\":\n",
    "            score -= 1\n",
    "        elif reply == \"undecidable\":\n",
    "            score += 0\n",
    "        else: \n",
    "            score += 0\n",
    "    return score\n",
    "\n",
    "def score_trs(attempt_list):\n",
    "    score = 0\n",
    "    for reply in attempt_list:\n",
    "        if reply == \"correct\":\n",
    "            score -= 1\n",
    "        elif reply == \"incorrect\":\n",
    "            score += 1\n",
    "        elif reply == \"undecidable\":\n",
    "            score += 0\n",
    "        else: \n",
    "            score += 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fail tests, Pass tests Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fail_pass_df(project, version, ts_id, prompt_no, example_no, sample_num, is_conv = False):\n",
    "    fail_info, fail_result, pass_info, pass_result = all_chat_reply_result(project, version, ts_id, prompt_no, example_no, is_conv)\n",
    "    # Create DataFrames for fail and pass results\n",
    "    fail_df = process_test_info_df(fail_info, fail_result, sample_num, \"incorrect\")\n",
    "    pass_df = process_test_info_df(pass_info, pass_result, sample_num, \"correct\")\n",
    "\n",
    "    fail_df[\"score_org\"] = fail_df[\"attempt_org\"].apply(score_org)\n",
    "    fail_df[\"score_trs\"] = fail_df[\"attempt_trs\"].apply(score_trs)\n",
    "    fail_df[\"score\"] = fail_df[\"score_org\"] + fail_df[\"score_trs\"]\n",
    "\n",
    "    pass_df[\"score_org\"] = pass_df[\"attempt_org\"].apply(score_org)\n",
    "    pass_df[\"score_trs\"] = pass_df[\"attempt_trs\"].apply(score_trs)\n",
    "    pass_df[\"score\"] = pass_df[\"score_org\"] + pass_df[\"score_trs\"]\n",
    "    return fail_df, pass_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For DATA ANALYSYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_countplot(ax, data, x, color):\n",
    "    a = sns.countplot(data=data, x=x, color=color)\n",
    "    plot_sum = 0\n",
    "    for p in a.patches:\n",
    "        height = p.get_height()\n",
    "        plot_sum += height\n",
    "    for p in a.patches:\n",
    "        height = p.get_height()\n",
    "        ax.annotate(f'{height}', (p.get_x() + p.get_width() / 2., height), ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=10)\n",
    "        #ax.annotate(f'{height/plot_sum * 100:.2f}', (p.get_x() + p.get_width() / 2., height), ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=10)\n",
    "    ax.set_xlabel(\"Predict\", fontsize=10)\n",
    "    ax.set_ylabel(\"number of test\", fontsize=10)\n",
    "\n",
    "def count_plot(fail_df, pass_df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Original Plots\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.title(\"Original_fail\", fontdict={'fontsize': 10})\n",
    "    plot_countplot(plt.gca(), fail_df, \"score_org\", '#4BC6CC')\n",
    "\n",
    "    plt.subplot(2, 3, 4)\n",
    "    plt.title(\"Original_pass\", fontdict={'fontsize': 10})\n",
    "    plot_countplot(plt.gca(), pass_df, \"score_org\", '#4BC6CC')\n",
    "\n",
    "    # Transformed Plots\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.title(\"Transformed_fail\", fontdict={'fontsize': 10})\n",
    "    plot_countplot(plt.gca(), fail_df, \"score_trs\", '#FF6E3E')\n",
    "\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.title(\"Transformed_pass\", fontdict={'fontsize': 10})\n",
    "    plot_countplot(plt.gca(), pass_df, \"score_trs\", '#FF6E3E')\n",
    "    # Sum Plots\n",
    "    fail_df[\"score\"] = fail_df[\"score_org\"] + fail_df[\"score_trs\"]\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.title(\"Sum[Incorrect Oracle]\", fontdict={'fontsize': 10})\n",
    "    plot_countplot(plt.gca(), fail_df, \"score\", 'dodgerblue')\n",
    "\n",
    "    pass_df[\"score\"] = pass_df[\"score_org\"] + pass_df[\"score_trs\"]\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.title(\"Sum[Correct Oracle]\", fontdict={'fontsize': 10})\n",
    "    plot_countplot(plt.gca(), pass_df, \"score\", 'dodgerblue')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def hist_plot(fail_df, pass_df):\n",
    "    fail_df[\"ground_truth\"] = \"incorrect\"\n",
    "    pass_df[\"ground_truth\"] = \"correct\"\n",
    "    pred_df = pd.concat([fail_df, pass_df])\n",
    "    sns.histplot(data=pred_df, x=\"score\", hue=\"ground_truth\", multiple=\"dodge\")\n",
    "    plt.yscale(\"log\")\n",
    "\n",
    "def accumulate_incorrect_ratio_graph(pred_df):\n",
    "    prediction_columns = [\"score\", \"score_org\", \"score_trs\"]\n",
    "    plt.figure(figsize=(13, 5))\n",
    "    for i, col in enumerate(prediction_columns):\n",
    "        # plt.subplot(1, len(prediction_columns), i+1)\n",
    "        thresholds = range(pred_df[col].min(), pred_df[col].max() +1)\n",
    "        incorrect_ratio = [(pred_df[pred_df[col] <= score].ground_truth == \"incorrect\").mean() for score in thresholds]\n",
    "        scores = [ 2 * th /(max(thresholds) - min(thresholds)) for th in thresholds]\n",
    "        # #scores = [ th for th in thresholds]\n",
    "        plt.plot(scores, incorrect_ratio, marker=\".\", label=col)\n",
    "\n",
    "    plt.hlines((pred_df.ground_truth == \"incorrect\").mean(), min(scores), max(scores), color=\"red\", label=\"% incorrect tests\")\n",
    "    plt.xlabel(\"th\")\n",
    "    plt.ylabel(\"% incorrect tests with score <= th\")\n",
    "    plt.legend()\n",
    "    plt.ylim((-0.05, 1.05))\n",
    "    plt.show()\n",
    "    \n",
    "def incorrect_ratio_graph(fail_df, pass_df, pred_df):\n",
    "    prediction_columns = [\"score\", \"score_org\", \"score_trs\"]\n",
    "    number_of_test_df = pd.DataFrame()\n",
    "    plt.figure(figsize=(13, 5))\n",
    "    for i, col in enumerate(prediction_columns):\n",
    "        number_of_test_df[f'{col}_failing_test'] = fail_df[col].value_counts()\n",
    "        number_of_test_df[f'{col}_passing_test'] = pass_df[col].value_counts()\n",
    "        number_of_test_df.fillna(0, inplace=True)\n",
    "        thresholds = range(pred_df[col].min(), pred_df[col].max() +1)\n",
    "        failing_ratio =  [number_of_test_df.loc[score, f'{col}_failing_test'] / (number_of_test_df.loc[score, f'{col}_failing_test'] + number_of_test_df.loc[score, f'{col}_passing_test']) for score in thresholds]\n",
    "        scores = [ 2 * th /(max(thresholds) - min(thresholds)) for th in thresholds]\n",
    "        plt.plot(scores, failing_ratio, marker=\".\", label=col)\n",
    "        \n",
    "    plt.hlines((pred_df.ground_truth == \"incorrect\").mean(), min(scores), max(scores), color=\"red\", label=\"% incorrect tests\")\n",
    "    plt.xlabel(\"score\")\n",
    "    plt.ylabel(\"% incorrect tests with score\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def precision_heatmap(pred_df):\n",
    "    rat_rows_precision = []\n",
    "    for org_score in range(-5, 6):\n",
    "        for trs_score in range(-5, 6):\n",
    "            # True Positives (TP): Ground truth is \"incorrect\" and predicted as \"incorrect\"\n",
    "            TP = ((pred_df.score_org <= org_score) & (pred_df.score_trs <= trs_score) & (pred_df.ground_truth == \"incorrect\")).sum()\n",
    "            # False Positives (FP): Ground truth is \"correct\" but predicted as \"incorrect\"\n",
    "            FP = ((pred_df.score_org <= org_score) & (pred_df.score_trs <= trs_score) & (pred_df.ground_truth == \"correct\")).sum()\n",
    "            # Calculate precision\n",
    "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "            rat_rows_precision.append((org_score, trs_score, precision))\n",
    "\n",
    "    # Create precision dataframe and heatmap\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    rat_df_precision = pd.DataFrame(data=rat_rows_precision, columns=[\"org_th\", \"trs_th\", \"precision\"]).pivot(\"org_th\", \"trs_th\", \"precision\")\n",
    "    sns.heatmap(rat_df_precision, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "    plt.xlabel(\"Threshold for Transformation Score\")\n",
    "    plt.ylabel(\"Threshold for Original Score\")\n",
    "    plt.title(\"Precision Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "def document_length_graph(pred_df):\n",
    "    score_doclen = []\n",
    "    for score in range(0, 11):\n",
    "        document_length = pred_df[(pred_df.score == score) | (pred_df.score == -score)].document_length.mean()\n",
    "        score_doclen.append((score, document_length))\n",
    "    score_doclen_df = pd.DataFrame(data=score_doclen, columns=[\"score\",\"doc_leng\"])\n",
    "    plt.plot(score_doclen_df[\"score\"],score_doclen_df[\"doc_leng\"], marker=\".\")\n",
    "    \n",
    "    print(\"doc_pearsonr:\",pearsonr(score_doclen_df[\"doc_leng\"], score_doclen_df[\"score\"]))\n",
    "    #sns.scatterplot(data= pred_df, x = \"score\", y = \"document_length\")\n",
    "    plt.xlabel(\"|score|\")\n",
    "    plt.ylabel(\"Document Length\")\n",
    "    plt.title(\"Correlation btw score and Doc_length\")\n",
    "\n",
    "def boxplot_pearsonr(pred_df):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.set_theme(style='whitegrid')\n",
    "    pred_df[\"is_correct\"] = pred_df[\"ground_truth\"] == \"correct\"\n",
    "    sns.boxplot(data=pred_df, x=\"ground_truth\", y=\"score\")\n",
    "    print(\"pearsonr:\",pearsonr(pred_df[\"is_correct\"], pred_df[\"score\"])) # point - biserial correlation coefficient\n",
    "\n",
    "def distplot(pred_df):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.displot(data=pred_df, hue=\"ground_truth\", x=\"score\", kde=True, multiple='dodge')\n",
    "\n",
    "def roc_auc(pred_df):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    pred_df.loc[pred_df[\"ground_truth\"]==\"incorrect\",[\"label\"]] = 1\n",
    "    pred_df.loc[pred_df[\"ground_truth\"]==\"correct\",[\"label\"]] = 0\n",
    "    suspiciousness = -(pred_df.score - pred_df.score.min())/(pred_df.score.max() - pred_df.score.min()) # 0 ~ 1\n",
    "    fprs, tprs, thresholds = roc_curve(pred_df.label.astype(bool), suspiciousness)\n",
    "    print('roc_acu_score:', roc_auc_score(pred_df.label.astype(bool), suspiciousness))\n",
    "    plt.plot(fprs, tprs, label = f'prompt')\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend()\n",
    "\n",
    "def prec_recall(pred_df):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    pred_scores = pred_df.score_org + pred_df.score_trs\n",
    "    suspiciousness = -(pred_scores - pred_scores.min())/(pred_scores.max() - pred_scores.min()) # 0 ~ 1\n",
    "    pre, rec, th = precision_recall_curve(pred_df.label.astype(bool), suspiciousness)\n",
    "    plt.plot(rec, pre, label = f'prompt')\n",
    "    plt.xlabel('rec')\n",
    "    plt.ylabel('pre')\n",
    "    plt.title(\"pre_rec Curve\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>version</th>\n",
       "      <th>idx</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>test_no</th>\n",
       "      <th>test_class</th>\n",
       "      <th>type</th>\n",
       "      <th>document_length</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>attempt_org</th>\n",
       "      <th>attempt_trs</th>\n",
       "      <th>score_org</th>\n",
       "      <th>score_trs</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>892</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test3</td>\n",
       "      <td>org.jfree.chart.util</td>\n",
       "      <td>assertEquals</td>\n",
       "      <td>141</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[undecidable, undecidable, incorrect, undecida...</td>\n",
       "      <td>[undecidable, incorrect, undecidable, incorrec...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>893</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test2</td>\n",
       "      <td>org.jfree.chart.util</td>\n",
       "      <td>assertEquals</td>\n",
       "      <td>141</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[incorrect, incorrect, incorrect, undecidable,...</td>\n",
       "      <td>[undecidable, incorrect, undecidable, incorrec...</td>\n",
       "      <td>-4</td>\n",
       "      <td>2</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>889</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test5</td>\n",
       "      <td>org.jfree.chart.util</td>\n",
       "      <td>assertEquals</td>\n",
       "      <td>141</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[undecidable, incorrect, incorrect, undecidabl...</td>\n",
       "      <td>[correct, undecidable, incorrect, undecidable,...</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>6667</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test5</td>\n",
       "      <td>org.jfree.data.statistics</td>\n",
       "      <td>assertTrue</td>\n",
       "      <td>151</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[undecidable, correct, correct, undecidable, i...</td>\n",
       "      <td>[correct, correct, undecidable, undecidable, u...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>6658</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test8</td>\n",
       "      <td>org.jfree.data.statistics</td>\n",
       "      <td>assertTrue</td>\n",
       "      <td>151</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[incorrect, incorrect, incorrect, incorrect, u...</td>\n",
       "      <td>[undecidable, undecidable, undecidable, incorr...</td>\n",
       "      <td>-4</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>2743</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test17</td>\n",
       "      <td>org.jfree.data.function</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>137</td>\n",
       "      <td>correct</td>\n",
       "      <td>[correct, correct, undecidable, undecidable, i...</td>\n",
       "      <td>[undecidable, undecidable, undecidable, incorr...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>2738</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test1</td>\n",
       "      <td>org.jfree.data.function</td>\n",
       "      <td>assertEquals</td>\n",
       "      <td>137</td>\n",
       "      <td>correct</td>\n",
       "      <td>[undecidable, undecidable, undecidable, undeci...</td>\n",
       "      <td>[undecidable, undecidable, incorrect, incorrec...</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>2744</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test7</td>\n",
       "      <td>org.jfree.data.function</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>137</td>\n",
       "      <td>correct</td>\n",
       "      <td>[correct, correct, undecidable, correct, correct]</td>\n",
       "      <td>[undecidable, undecidable, incorrect, incorrec...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>2740</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test17</td>\n",
       "      <td>org.jfree.data.function</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>137</td>\n",
       "      <td>correct</td>\n",
       "      <td>[undecidable, correct, correct, undecidable, u...</td>\n",
       "      <td>[undecidable, undecidable, correct, undecidabl...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Chart</td>\n",
       "      <td>1</td>\n",
       "      <td>2740</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test7</td>\n",
       "      <td>org.jfree.data.function</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>137</td>\n",
       "      <td>correct</td>\n",
       "      <td>[correct, correct, correct, correct, correct]</td>\n",
       "      <td>[correct, correct, correct, correct, correct]</td>\n",
       "      <td>5</td>\n",
       "      <td>-5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>888 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    project version   idx      ts_id test_no                 test_class  \\\n",
       "0     Chart       1   892  newTS_300   test3       org.jfree.chart.util   \n",
       "1     Chart       1   893  newTS_300   test2       org.jfree.chart.util   \n",
       "2     Chart       1   889  newTS_300   test5       org.jfree.chart.util   \n",
       "3     Chart       1  6667  newTS_300   test5  org.jfree.data.statistics   \n",
       "4     Chart       1  6658  newTS_300   test8  org.jfree.data.statistics   \n",
       "..      ...     ...   ...        ...     ...                        ...   \n",
       "439   Chart       1  2743  newTS_300  test17    org.jfree.data.function   \n",
       "440   Chart       1  2738  newTS_300   test1    org.jfree.data.function   \n",
       "441   Chart       1  2744  newTS_300   test7    org.jfree.data.function   \n",
       "442   Chart       1  2740  newTS_300  test17    org.jfree.data.function   \n",
       "443   Chart       1  2740  newTS_300   test7    org.jfree.data.function   \n",
       "\n",
       "             type  document_length ground_truth  \\\n",
       "0    assertEquals              141    incorrect   \n",
       "1    assertEquals              141    incorrect   \n",
       "2    assertEquals              141    incorrect   \n",
       "3      assertTrue              151    incorrect   \n",
       "4      assertTrue              151    incorrect   \n",
       "..            ...              ...          ...   \n",
       "439   assertFalse              137      correct   \n",
       "440  assertEquals              137      correct   \n",
       "441   assertFalse              137      correct   \n",
       "442   assertFalse              137      correct   \n",
       "443   assertFalse              137      correct   \n",
       "\n",
       "                                           attempt_org  \\\n",
       "0    [undecidable, undecidable, incorrect, undecida...   \n",
       "1    [incorrect, incorrect, incorrect, undecidable,...   \n",
       "2    [undecidable, incorrect, incorrect, undecidabl...   \n",
       "3    [undecidable, correct, correct, undecidable, i...   \n",
       "4    [incorrect, incorrect, incorrect, incorrect, u...   \n",
       "..                                                 ...   \n",
       "439  [correct, correct, undecidable, undecidable, i...   \n",
       "440  [undecidable, undecidable, undecidable, undeci...   \n",
       "441  [correct, correct, undecidable, correct, correct]   \n",
       "442  [undecidable, correct, correct, undecidable, u...   \n",
       "443      [correct, correct, correct, correct, correct]   \n",
       "\n",
       "                                           attempt_trs  score_org  score_trs  \\\n",
       "0    [undecidable, incorrect, undecidable, incorrec...         -1          3   \n",
       "1    [undecidable, incorrect, undecidable, incorrec...         -4          2   \n",
       "2    [correct, undecidable, incorrect, undecidable,...         -2          0   \n",
       "3    [correct, correct, undecidable, undecidable, u...          1         -2   \n",
       "4    [undecidable, undecidable, undecidable, incorr...         -4          1   \n",
       "..                                                 ...        ...        ...   \n",
       "439  [undecidable, undecidable, undecidable, incorr...          1          2   \n",
       "440  [undecidable, undecidable, incorrect, incorrec...         -1          3   \n",
       "441  [undecidable, undecidable, incorrect, incorrec...          4          1   \n",
       "442  [undecidable, undecidable, correct, undecidabl...          2         -1   \n",
       "443      [correct, correct, correct, correct, correct]          5         -5   \n",
       "\n",
       "     score  \n",
       "0        2  \n",
       "1       -2  \n",
       "2       -2  \n",
       "3       -1  \n",
       "4       -3  \n",
       "..     ...  \n",
       "439      3  \n",
       "440      2  \n",
       "441      5  \n",
       "442      1  \n",
       "443      0  \n",
       "\n",
       "[888 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project = \"Chart\"\n",
    "version = \"1\"\n",
    "ts_id = \"newTS_300\"\n",
    "prompt_no = 8\n",
    "example_num = 0\n",
    "sample_num = 5\n",
    "result = []\n",
    "\n",
    "fail_df, pass_df = get_fail_pass_df(project, version, ts_id, prompt_no, example_num, sample_num, is_conv = True)\n",
    "pred_df = pd.concat([fail_df, pass_df])\n",
    "display(pred_df)\n",
    "\n",
    "with open(f'{project}_Dataframe_3.pkl','wb') as f:\n",
    "     pickle.dump(pred_df, f)\n",
    "\n",
    "# count_plot(fail_df=fail_df, pass_df=pass_df)\n",
    "# hist_plot(fail_df=fail_df, pass_df=pass_df) \n",
    "# accumulate_incorrect_ratio_graph(pred_df)\n",
    "# incorrect_ratio_graph(fail_df, pass_df, pred_df)\n",
    "# precision_heatmap(pred_df)\n",
    "# document_length_graph(pred_df)\n",
    "# distplot(pred_df)\n",
    "# print(\"prompt_no:\",prompt_no, \"sample_num:\", sample_num)\n",
    "# boxplot_pearsonr(pred_df)\n",
    "# roc_auc(pred_df)\n",
    "# prec_recall(pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ1. Score Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project</th>\n",
       "      <th>version</th>\n",
       "      <th>idx</th>\n",
       "      <th>ts_id</th>\n",
       "      <th>test_no</th>\n",
       "      <th>test_class</th>\n",
       "      <th>type</th>\n",
       "      <th>document_length</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>attempt_org</th>\n",
       "      <th>attempt_trs</th>\n",
       "      <th>score_org</th>\n",
       "      <th>score_trs</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test0</td>\n",
       "      <td>com.google.javascript.rhino</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>146</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[undecidable, undecidable, undecidable, undeci...</td>\n",
       "      <td>[undecidable, undecidable, undecidable, undeci...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>224</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test3</td>\n",
       "      <td>com.google.javascript.rhino</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>146</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[undecidable, undecidable, undecidable, correc...</td>\n",
       "      <td>[undecidable, undecidable, undecidable, undeci...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test4</td>\n",
       "      <td>com.google.javascript.rhino</td>\n",
       "      <td>assertTrue</td>\n",
       "      <td>146</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[undecidable, undecidable, undecidable, undeci...</td>\n",
       "      <td>[undecidable, undecidable, incorrect, undecida...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>225</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test5</td>\n",
       "      <td>com.google.javascript.rhino</td>\n",
       "      <td>assertTrue</td>\n",
       "      <td>146</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[undecidable, undecidable, undecidable, undeci...</td>\n",
       "      <td>[undecidable, undecidable, incorrect, undecida...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>227</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test4</td>\n",
       "      <td>com.google.javascript.rhino</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>146</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>[undecidable, undecidable, incorrect, incorrec...</td>\n",
       "      <td>[incorrect, undecidable, undecidable, undecida...</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test2</td>\n",
       "      <td>com.google.javascript.jscomp</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>538</td>\n",
       "      <td>correct</td>\n",
       "      <td>[correct, correct, correct, correct, correct]</td>\n",
       "      <td>[incorrect, correct, incorrect, incorrect, inc...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test0</td>\n",
       "      <td>com.google.javascript.jscomp</td>\n",
       "      <td>assertTrue</td>\n",
       "      <td>538</td>\n",
       "      <td>correct</td>\n",
       "      <td>[undecidable, correct, undecidable, correct, u...</td>\n",
       "      <td>[undecidable, correct, undecidable, correct, c...</td>\n",
       "      <td>2</td>\n",
       "      <td>-3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test3</td>\n",
       "      <td>com.google.javascript.jscomp</td>\n",
       "      <td>assertFalse</td>\n",
       "      <td>538</td>\n",
       "      <td>correct</td>\n",
       "      <td>[undecidable, correct, correct, correct, correct]</td>\n",
       "      <td>[incorrect, incorrect, incorrect, incorrect, i...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test3</td>\n",
       "      <td>com.google.javascript.jscomp</td>\n",
       "      <td>assertTrue</td>\n",
       "      <td>538</td>\n",
       "      <td>correct</td>\n",
       "      <td>[correct, correct, undecidable, correct, incor...</td>\n",
       "      <td>[correct, correct, correct, correct, correct]</td>\n",
       "      <td>2</td>\n",
       "      <td>-5</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Lang</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>newTS_300</td>\n",
       "      <td>test1</td>\n",
       "      <td>com.google.javascript.jscomp</td>\n",
       "      <td>assertNotNull</td>\n",
       "      <td>238</td>\n",
       "      <td>correct</td>\n",
       "      <td>[incorrect, undecidable, correct, undecidable,...</td>\n",
       "      <td>[incorrect, incorrect, incorrect, undecidable,...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   project version  idx      ts_id test_no                    test_class  \\\n",
       "0     Lang       1  223  newTS_300   test0   com.google.javascript.rhino   \n",
       "1     Lang       1  224  newTS_300   test3   com.google.javascript.rhino   \n",
       "2     Lang       1  225  newTS_300   test4   com.google.javascript.rhino   \n",
       "3     Lang       1  225  newTS_300   test5   com.google.javascript.rhino   \n",
       "4     Lang       1  227  newTS_300   test4   com.google.javascript.rhino   \n",
       "..     ...     ...  ...        ...     ...                           ...   \n",
       "44    Lang       1   94  newTS_300   test2  com.google.javascript.jscomp   \n",
       "45    Lang       1   93  newTS_300   test0  com.google.javascript.jscomp   \n",
       "46    Lang       1   90  newTS_300   test3  com.google.javascript.jscomp   \n",
       "47    Lang       1   91  newTS_300   test3  com.google.javascript.jscomp   \n",
       "48    Lang       1   84  newTS_300   test1  com.google.javascript.jscomp   \n",
       "\n",
       "             type  document_length ground_truth  \\\n",
       "0     assertFalse              146    incorrect   \n",
       "1     assertFalse              146    incorrect   \n",
       "2      assertTrue              146    incorrect   \n",
       "3      assertTrue              146    incorrect   \n",
       "4     assertFalse              146    incorrect   \n",
       "..            ...              ...          ...   \n",
       "44    assertFalse              538      correct   \n",
       "45     assertTrue              538      correct   \n",
       "46    assertFalse              538      correct   \n",
       "47     assertTrue              538      correct   \n",
       "48  assertNotNull              238      correct   \n",
       "\n",
       "                                          attempt_org  \\\n",
       "0   [undecidable, undecidable, undecidable, undeci...   \n",
       "1   [undecidable, undecidable, undecidable, correc...   \n",
       "2   [undecidable, undecidable, undecidable, undeci...   \n",
       "3   [undecidable, undecidable, undecidable, undeci...   \n",
       "4   [undecidable, undecidable, incorrect, incorrec...   \n",
       "..                                                ...   \n",
       "44      [correct, correct, correct, correct, correct]   \n",
       "45  [undecidable, correct, undecidable, correct, u...   \n",
       "46  [undecidable, correct, correct, correct, correct]   \n",
       "47  [correct, correct, undecidable, correct, incor...   \n",
       "48  [incorrect, undecidable, correct, undecidable,...   \n",
       "\n",
       "                                          attempt_trs  score_org  score_trs  \\\n",
       "0   [undecidable, undecidable, undecidable, undeci...          0          1   \n",
       "1   [undecidable, undecidable, undecidable, undeci...          1         -1   \n",
       "2   [undecidable, undecidable, incorrect, undecida...          0          1   \n",
       "3   [undecidable, undecidable, incorrect, undecida...          0          1   \n",
       "4   [incorrect, undecidable, undecidable, undecida...         -2          1   \n",
       "..                                                ...        ...        ...   \n",
       "44  [incorrect, correct, incorrect, incorrect, inc...          5          3   \n",
       "45  [undecidable, correct, undecidable, correct, c...          2         -3   \n",
       "46  [incorrect, incorrect, incorrect, incorrect, i...          4          5   \n",
       "47      [correct, correct, correct, correct, correct]          2         -5   \n",
       "48  [incorrect, incorrect, incorrect, undecidable,...          0          4   \n",
       "\n",
       "    score  \n",
       "0       1  \n",
       "1       0  \n",
       "2       1  \n",
       "3       1  \n",
       "4      -1  \n",
       "..    ...  \n",
       "44      8  \n",
       "45     -1  \n",
       "46      9  \n",
       "47     -3  \n",
       "48      4  \n",
       "\n",
       "[98 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# project = 'Math'\n",
    "# with open('/home/coinse/Workspace/autooracle/workspace/src/analyze/Closure_Dataframe.pkl','rb') as f:\n",
    "#      data = pickle.load(f)\n",
    "\n",
    "# display(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'/usr/bin/python3.8'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: '/usr/bin/python3.8 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "fail_df[\"score\"] = fail_df.score_org + fail_df.score_trs\n",
    "pass_df[\"score\"] = pass_df.score_org + pass_df.score_trs\n",
    "\n",
    "pred_df = pd.concat([fail_df, pass_df])\n",
    "prediction_columns = [\"score\", \"score_org\", \"score_trs\"]\n",
    "plt.figure(figsize=(13, 5))\n",
    "for i, col in enumerate(prediction_columns):\n",
    "    # plt.subplot(1, len(prediction_columns), i+1)\n",
    "    thresholds = range(pred_df[col].min(), pred_df[col].max() +1)\n",
    "    incorrect_ratio = [(pred_df[pred_df[col] <= score].ground_truth == \"incorrect\").mean() for score in thresholds]\n",
    "    scores = [ 2 * th /(max(thresholds) - min(thresholds)) for th in thresholds]\n",
    "    # #scores = [ th for th in thresholds]\n",
    "    plt.plot(scores, incorrect_ratio, marker=\".\", label=col)\n",
    "\n",
    "plt.hlines((pred_df.ground_truth == \"incorrect\").mean(), min(scores), max(scores), color=\"red\", label=\"% incorrect tests\")\n",
    "plt.xlabel(\"th\")\n",
    "plt.ylabel(\"% incorrect tests with score <= th\")\n",
    "plt.legend()\n",
    "plt.ylim((-0.05, 1.05))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "number_of_test_df = pd.DataFrame()\n",
    "plt.figure(figsize=(13, 5))\n",
    "for i, col in enumerate(prediction_columns):\n",
    "    number_of_test_df[f'{col}_failing_test'] = fail_df[col].value_counts()\n",
    "    number_of_test_df[f'{col}_passing_test'] = pass_df[col].value_counts()\n",
    "    number_of_test_df.fillna(0, inplace=True)\n",
    "\n",
    "    thresholds = range(pred_df[col].min(), pred_df[col].max() +1)\n",
    "    failing_ratio =  [number_of_test_df.loc[score, f'{col}_failing_test'] / (number_of_test_df.loc[score, f'{col}_failing_test'] + number_of_test_df.loc[score, f'{col}_passing_test']) for score in thresholds]\n",
    "    scores = [ 2 * th /(max(thresholds) - min(thresholds)) for th in thresholds]\n",
    "    plt.plot(scores, failing_ratio, marker=\".\", label=col)\n",
    "    \n",
    "plt.hlines((pred_df.ground_truth == \"incorrect\").mean(), min(scores), max(scores), color=\"red\", label=\"% incorrect tests\")\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"% incorrect tests with score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# rat_rows_precision = []\n",
    "# for org_score in range(-5, 6):\n",
    "#     for trs_score in range(-5, 6):\n",
    "#         # True Positives (TP): Ground truth is \"incorrect\" and predicted as \"incorrect\"\n",
    "#         TP = ((pred_df.score_org <= org_score) & (pred_df.score_trs <= trs_score) & (pred_df.ground_truth == \"incorrect\")).sum()\n",
    "#         # False Positives (FP): Ground truth is \"correct\" but predicted as \"incorrect\"\n",
    "#         FP = ((pred_df.score_org <= org_score) & (pred_df.score_trs <= trs_score) & (pred_df.ground_truth == \"correct\")).sum()\n",
    "#         # Calculate precision\n",
    "#         precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "#         rat_rows_precision.append((org_score, trs_score, precision))\n",
    "\n",
    "# # Create precision dataframe and heatmap\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# rat_df_precision = pd.DataFrame(data=rat_rows_precision, columns=[\"org_th\", \"trs_th\", \"precision\"]).pivot(\"org_th\", \"trs_th\", \"precision\")\n",
    "# sns.heatmap(rat_df_precision, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "# plt.xlabel(\"Threshold for Transformation Score\")\n",
    "# plt.ylabel(\"Threshold for Original Score\")\n",
    "# plt.title(\"Precision Heatmap\")\n",
    "# plt.show()\n",
    "\n",
    "# Recall \n",
    "total_incorrect = (pred_df.ground_truth == \"incorrect\").sum()\n",
    "print((pred_df.score < 0).sum()/ total_incorrect)\n",
    "plt.figure(figsize=(13, 5))\n",
    "\n",
    "score_doclen = []\n",
    "for score in range(-10, 11):\n",
    "    document_length = pred_df[pred_df.score == score].document_length.mean()\n",
    "    score_doclen.append((score, document_length))\n",
    "score_doclen_df = pd.DataFrame(data=score_doclen, columns=[\"score\",\"doc_leng\"])\n",
    "plt.plot(score_doclen_df[\"score\"],score_doclen_df[\"doc_leng\"], marker=\".\")\n",
    "plt.xlabel(\"score\")\n",
    "plt.ylabel(\"Document Length\")\n",
    "plt.title(\"Correlation btw score and Doc_length\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'/usr/bin/python3.8'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: '/usr/bin/python3.8 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme(style='whitegrid')\n",
    "pred_df\n",
    "\n",
    "pred_df[\"is_correct\"] = pred_df[\"ground_truth\"] == \"correct\"\n",
    "sns.boxplot(data=pred_df, x=\"ground_truth\", y=\"score\")\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pearsonr(pred_df[\"is_correct\"], pred_df[\"score\"]) # point - biserial correlation coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'/usr/bin/python3.8'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: '/usr/bin/python3.8 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "sns.displot(data=pred_df, hue=\"ground_truth\", x=\"score\", kde=True, multiple='dodge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'/usr/bin/python3.8'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: '/usr/bin/python3.8 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def count_occurrences(attempt_list, label):\n",
    "    return attempt_list.count(label)\n",
    "\n",
    "project = \"Lang\"\n",
    "version = \"1\"\n",
    "ts_id = \"newTS_300\"\n",
    "example_num = 0\n",
    "\n",
    "dfs = []\n",
    "for prompt_no in [8, 9, 10 ,11]:\n",
    "    fail_df, pass_df = get_fail_pass_df(project, version, ts_id, prompt_no, example_num, is_conv = True)\n",
    "    fail_df[\"score\"] = fail_df.score_org + fail_df.score_trs\n",
    "    pass_df[\"score\"] = pass_df.score_org + pass_df.score_trs\n",
    "    pred_df = pd.concat([fail_df, pass_df])\n",
    "    labels = [\"correct\", \"undecidable\", \"incorrect\"]\n",
    "    pred_df_trys = pred_df.loc[:,[\"attempt_org\", \"attempt_trs\"]]\n",
    "    for label in labels:\n",
    "        pred_df[f\"attempt_org_{label}_cnt\"] = pred_df_trys[\"attempt_org\"].apply(lambda x: count_occurrences(x, label))\n",
    "        pred_df[f\"attempt_trs_{label}_cnt\"] = pred_df_trys[\"attempt_trs\"].apply(lambda x: count_occurrences(x, label))\n",
    "\n",
    "    pred_df.loc[pred_df[\"ground_truth\"]==\"incorrect\",[\"label\"]] = 1\n",
    "    pred_df.loc[pred_df[\"ground_truth\"]==\"correct\",[\"label\"]] = 0\n",
    "    dfs.append(pred_df)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    pred_scores = df.score_org + df.score_trs\n",
    "    suspiciousness = -(pred_scores - pred_scores.min())/(pred_scores.max() - pred_scores.min()) # 0 ~ 1\n",
    "    fprs, tprs, thresholds = roc_curve(df.label.astype(bool), suspiciousness)\n",
    "    pre, rec,th = precision_recall_curve(df.label.astype(bool), suspiciousness)\n",
    "    print('roc_acut_score:', roc_auc_score(df.label.astype(bool), suspiciousness))\n",
    "    #plt.plot(fprs, tprs, label = f'prompt_{i+1}')\n",
    "    plt.plot(rec, pre, label = f'prompt_{i+1}')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (deprecated) Document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'/usr/bin/python3.8'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: '/usr/bin/python3.8 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# length = {}\n",
    "# for score in range(pred_df[\"Predict\"].min(), pred_df[\"Predict\"].max() + 1):\n",
    "#     length[score]=pred_df[pred_df[\"Predict\"]==score].document_length.mean()\n",
    "\n",
    "# result = pd.Series(data= length ).to_frame().rename(columns={\"0\":\"length\"})\n",
    "\n",
    "# result.rename(columns={0:\"length\"}, inplace=True)\n",
    "# result\n",
    "# plt.title(\"Correlation btw documnet_length and prediction score\")\n",
    "# plt.plot(result.index, result.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'/usr/bin/python3.8'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: '/usr/bin/python3.8 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "pred_df.loc[pred_df[\"ground_truth\"]==\"incorrect\",[\"label\"]] = 1 \n",
    "pred_df.loc[pred_df[\"ground_truth\"]==\"correct\",[\"label\"]] = 0\n",
    "\n",
    "def score_org(attempt_list):\n",
    "    score = 0\n",
    "    for reply in attempt_list:\n",
    "        if reply == \"correct\":\n",
    "            score += 1\n",
    "        elif reply == \"incorrect\":\n",
    "            score -= 1\n",
    "        elif reply == \"undecidable\":\n",
    "            score += 0\n",
    "        else: \n",
    "            score += 0\n",
    "    return score\n",
    "\n",
    "def count_occurrences(attempt_list, label):\n",
    "    return attempt_list.count(label)\n",
    "\n",
    "labels = [\"correct\", \"undecidable\", \"incorrect\"]\n",
    "pred_df_trys = pred_df.loc[:,[\"attempt_org\", \"attempt_trs\"]]\n",
    "for label in labels:\n",
    "    pred_df[f\"attempt_org_{label}_cnt\"] = pred_df_trys[\"attempt_org\"].apply(lambda x: count_occurrences(x, label))\n",
    "    pred_df[f\"attempt_trs_{label}_cnt\"] = pred_df_trys[\"attempt_trs\"].apply(lambda x: count_occurrences(x, label))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, auc\n",
    "X_train, X_test, y_train, y_test = train_test_split(pred_df[[\"score_org\", \"score_trs\"]], pred_df[\"label\"], test_size=0.25, stratify=pred_df[\"label\"])\n",
    "\n",
    "scaler = MaxAbsScaler()\n",
    "s_X_train = scaler.fit_transform(X_train)\n",
    "model = LogisticRegression(class_weight=\"balanced\")\n",
    "\n",
    "model.fit(s_X_train,y_train)\n",
    "\n",
    "print(model.score(s_X_train, y_train))\n",
    "\n",
    "# y_pred = model.predict(scaler.transform(X_test))\n",
    "# #print(y_test.mean(), precision_score(y_true=y_test, y_pred=y_pred), recall_score(y_true=y_test, y_pred=y_pred))\n",
    "# prec, rec, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "# #print(auc(rec, prec))\n",
    "# plt.plot(rec, prec, label = 'Logistic Regression')\n",
    "\n",
    "pred_scores = X_test.score_org + X_test.score_trs\n",
    "suspiciousness = (pred_scores - pred_scores.min())/(pred_scores.max() - pred_scores.min()) # 0 ~ 1\n",
    "print(suspiciousness)\n",
    "prec, rec, thresholds = precision_recall_curve(y_test.astype(bool), suspiciousness)\n",
    "# print(thresholds)\n",
    "# print(auc(rec, prec))\n",
    "plt.plot(rec, prec, label = \"Score based\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.legend()\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MaxAbsScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import precision_score, recall_score, precision_recall_curve, auc, roc_curve\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(pred_df[[\"attempt_org_correct_cnt\", \"attempt_org_undecidable_cnt\", \"attempt_org_incorrect_cnt\", \"attempt_trs_correct_cnt\", \"attempt_trs_undecidable_cnt\", \"attempt_trs_incorrect_cnt\"]], pred_df[\"label\"], test_size=0.25, stratify=pred_df[\"label\"])\n",
    "# scaler = MaxAbsScaler()\n",
    "# s_X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# model = LogisticRegression(class_weight=\"balanced\")\n",
    "# model.fit(s_X_train,y_train)\n",
    "# y_pred = model.predict(scaler.transform(X_test))\n",
    "\n",
    "# print(\"model.score:\",model.score(s_X_train, y_train))\n",
    "# print(precision_score(y_true=y_test, y_pred=y_pred), recall_score(y_true=y_test, y_pred=y_pred))\n",
    "# prec, rec, thresholds = precision_recall_curve(y_test, y_pred)\n",
    "# print(prec, rec, thresholds)\n",
    "# print(auc(rec, prec))\n",
    "# plt.title(\"recall_precision curve\")\n",
    "# plt.plot(rec, prec)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
