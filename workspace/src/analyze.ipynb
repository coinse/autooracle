{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd \n",
    "import pickle\n",
    "import os \n",
    "from utils.env import EvoD4jEnv\n",
    "import re \n",
    "\n",
    "project = \"Lang\"\n",
    "version = \"\"\n",
    "ts_id = \"newTS\"\n",
    "prompt_no = 1\n",
    "example_num = 2\n",
    "try_no = \"\"\n",
    "mut = \"\"\n",
    "\n",
    "def class_name_to_test_path(class_name):\n",
    "    return class_name.replace('.', '/')+'.java'\n",
    "\n",
    "\n",
    "# Get Ground Truth\n",
    "evo_test_df_all = [] \n",
    "# To make it start with index 1\n",
    "evo_test_df_all.append(pd.DataFrame())\n",
    "for i in range(1, 66):\n",
    "    if i == 2 or i == 30 or i == 63:\n",
    "        evo_test_df_all.append(pd.DataFrame())\n",
    "        continue # deprecated version\n",
    "    env = EvoD4jEnv(project, i, ts_id)\n",
    "    # Original\n",
    "    evo_test_path = env.evosuite_test_dir.replace(\"/root/workspace\",\"/Users/hslee/Workspace/AutoOracle/workspace\")\n",
    "    with open(os.path.join(evo_test_path, \"evo_tests_df.pkl\"),'rb') as f:\n",
    "        evo_test_df = pickle.load(f)\n",
    "        evo_test_df[\"ground_truth\"] = \"correct\"\n",
    "    # Filtering the failing tests\n",
    "    with open(os.path.join(evo_test_path, \"failing_tests_on_fixed\")) as f:\n",
    "        data = f.readlines()\n",
    "    for l in data:\n",
    "        if l.startswith('---'):\n",
    "            pattern = r\"--- (.*?)::(.*?)$\"\n",
    "            match = re.search(pattern, l)\n",
    "            if match:\n",
    "                class_name = match.group(1)\n",
    "                class_name = class_name.replace('.', '/')+'.java'\n",
    "                test_no = match.group(2)\n",
    "            evo_test_df.loc[(evo_test_df[\"evo_relpath\"] == class_name) & (evo_test_df[\"evo_test_no\"] == test_no),\"ground_truth\"] = \"incorrect\"\n",
    "    evo_test_df_all.append(evo_test_df)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Chat_Reply \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Chat_reply with prompt 1 \n",
    "for i in range(1, 66):\n",
    "    if i == 2 or i==30 or i == 63 :\n",
    "        continue # deprecated version or compilation error\n",
    "    env = EvoD4jEnv(project, i, ts_id)\n",
    "    chat_reply_dir_root = env.evosuite_chat_reply_dir.replace(\"/root/workspace\",\"/Users/hslee/Workspace/AutoOracle/workspace\")\n",
    "\n",
    "    #with prompt1 & example2\n",
    "    for j in range(1,4):\n",
    "        chat_reply_dir = os.path.join(chat_reply_dir_root,\"prompt1/example2/try{}\".format(j))\n",
    "        chat_reply_list = os.listdir(chat_reply_dir)\n",
    "\n",
    "        for chat_reply_f in chat_reply_list:\n",
    "            with open(os.path.join(chat_reply_dir, chat_reply_f),'r') as fr:\n",
    "                data = fr.read()\n",
    "            split_chat_reply_f = chat_reply_f.strip().split('_')\n",
    "            dir = split_chat_reply_f[0].replace('.','/')\n",
    "            test_no = split_chat_reply_f[1]\n",
    "            df = evo_test_df_all[i]\n",
    "            if re.search(\"incorrect\", data):\n",
    "                df.loc[ (df[\"dir\"] == dir) & (df[\"evo_test_no\"] == test_no), \"predict{}\".format(j) ] = \"incorrect\"\n",
    "            elif re.search(\"correct\", data):\n",
    "                df.loc[ (df[\"dir\"] == dir) & (df[\"evo_test_no\"] == test_no), \"predict{}\".format(j)  ] = \"correct\" \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************\n",
      "Result of prediction 1\n",
      "***********************\n",
      "True Negative: 24\n",
      "False Positive: 20\n",
      "False Negative: 836\n",
      "True Positive: 1016\n",
      "\n",
      "accuracy: 0.5485232067510548\n",
      "precision: 0.9806949806949807\n",
      "recall: 0.5485961123110151\n",
      "f1: 0.7036011080332409\n",
      " \n",
      "***********************\n",
      "Result of prediction 2\n",
      "***********************\n",
      "True Negative: 28\n",
      "False Positive: 16\n",
      "False Negative: 866\n",
      "True Positive: 986\n",
      "\n",
      "accuracy: 0.5348101265822784\n",
      "precision: 0.9840319361277445\n",
      "recall: 0.5323974082073434\n",
      "f1: 0.6909600560616678\n",
      " \n",
      "***********************\n",
      "Result of prediction 3\n",
      "***********************\n",
      "True Negative: 26\n",
      "False Positive: 18\n",
      "False Negative: 855\n",
      "True Positive: 997\n",
      "\n",
      "accuracy: 0.5395569620253164\n",
      "precision: 0.9822660098522168\n",
      "recall: 0.5383369330453563\n",
      "f1: 0.6955005231949773\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "for j in range(1,4):\n",
    "    ground_truth =[]\n",
    "    predict=[]\n",
    "    print(\"***********************\")\n",
    "    print(\"Result of prediction {}\".format(j))\n",
    "    print(\"***********************\")\n",
    "    for i in range(1, 66):\n",
    "        if i == 2 or i==30 or i == 63 :\n",
    "            continue # deprecated version or compilation error\n",
    "        tmp_ground_truth = evo_test_df_all[i][\"ground_truth\"].values.tolist()\n",
    "        tmp_ground_truth = [ 1 if x == \"correct\" else 0 for x in tmp_ground_truth]\n",
    "        tmp_predict = evo_test_df_all[i][\"predict{}\".format(j)].values.tolist()\n",
    "        tmp_predict = [ 1 if x == \"correct\" else 0 for x in tmp_predict]\n",
    "        ground_truth.extend(tmp_ground_truth)\n",
    "        predict.extend(tmp_predict)\n",
    "\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(ground_truth, predict).ravel()\n",
    "\n",
    "    print(\"True Negative:\", tn)\n",
    "    print(\"False Positive:\", fp)\n",
    "    print(\"False Negative:\", fn)\n",
    "    print(\"True Positive:\", tp)\n",
    "    print()\n",
    "    accuracy = metrics.accuracy_score(ground_truth, predict)\n",
    "    print(\"accuracy:\", accuracy)\n",
    "\n",
    "    precision = metrics.precision_score(ground_truth, predict)\n",
    "    print(\"precision:\", precision)\n",
    "\n",
    "    recall = metrics.recall_score(ground_truth, predict)\n",
    "    print(\"recall:\", recall)\n",
    "\n",
    "\n",
    "    f1 = metrics.f1_score(ground_truth, predict)\n",
    "    print(\"f1:\", f1)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************\n",
      "Consistency_Original\n",
      "***********************\n",
      "True Negative: 11\n",
      "False Positive: 3\n",
      "False Negative: 191\n",
      "True Positive: 591\n",
      "accuracy: 0.7562814070351759\n",
      "precision: 0.9949494949494949\n",
      "recall: 0.7557544757033248\n",
      "f1: 0.8590116279069767\n",
      "796\n"
     ]
    }
   ],
   "source": [
    "print(\"***********************\")\n",
    "print(\"Consistency_Original\")\n",
    "print(\"***********************\")\n",
    "ground_truth =[]\n",
    "predict=[]\n",
    "for i in range(1, 66):\n",
    "    if i == 2 or i==30 or i == 63 :\n",
    "        continue # deprecated version or compilation error\n",
    "    evo_test_df_all[i] = evo_test_df_all[i].loc[evo_test_df_all[i][\"predict1\"] == evo_test_df_all[i][\"predict2\"],:]\n",
    "    evo_test_df_all[i] = evo_test_df_all[i].loc[evo_test_df_all[i][\"predict2\"] == evo_test_df_all[i][\"predict3\"],:]\n",
    "    tmp_ground_truth = evo_test_df_all[i][\"ground_truth\"].values.tolist()\n",
    "    tmp_ground_truth = [ 1 if x == \"correct\" else 0 for x in tmp_ground_truth]\n",
    "    tmp_predict1 = evo_test_df_all[i][\"predict1\"].values.tolist()\n",
    "    tmp_predict2 = evo_test_df_all[i][\"predict2\"].values.tolist()\n",
    "    tmp_predict3 = evo_test_df_all[i][\"predict3\"].values.tolist()\n",
    "    tmp_predict = [ 1 if x == (\"correct\",\"correct\",\"correct\") else 0 for x in zip(tmp_predict1,tmp_predict2,tmp_predict3)]\n",
    "    ground_truth.extend(tmp_ground_truth)\n",
    "    predict.extend(tmp_predict)\n",
    "\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(ground_truth, predict).ravel()\n",
    "\n",
    "print(\"True Negative:\", tn)\n",
    "print(\"False Positive:\", fp)\n",
    "print(\"False Negative:\", fn)\n",
    "print(\"True Positive:\", tp)\n",
    "\n",
    "accuracy = metrics.accuracy_score(ground_truth, predict)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "precision = metrics.precision_score(ground_truth, predict)\n",
    "print(\"precision:\", precision)\n",
    "\n",
    "recall = metrics.recall_score(ground_truth, predict)\n",
    "print(\"recall:\", recall)\n",
    "\n",
    "f1 = metrics.f1_score(ground_truth, predict)\n",
    "print(\"f1:\", f1)\n",
    "\n",
    "print(tn+ fp+ fn+ tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
